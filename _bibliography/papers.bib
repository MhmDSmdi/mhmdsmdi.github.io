---
---

@string{aps = {American Physical Society,}}

@inproceedings{robustEM,
    preview = {cikm22.jpg},
    author = {{Akbarian Rastaghi}, Mehdi and {Kamalloo}, Ehsan and {Rafiei}, Davood},
    title = {Probing the Robustness of Pre-trained Language Models for Entity Matching},
    year = {2022},
    publisher = {Association for Computing Machinery},
    abstract = {The paradigm of fine-tuning Pre-trained Language Models (PLMs) has been successful in Entity Matching (EM). Despite their remarkable performance, PLMs exhibit tendency to learn spurious correlations from training data. In this work, we aim at investigating whether PLM-based entity matching models can be trusted in real-world applications where data distribution is different from that of training. To this end, we design an evaluation benchmark to assess the robustness of EM models to facilitate their deployment in the real-world settings. Our assessments reveal that data imbalance in the training data is a key problem for robustness. We also find that data augmentation alone is not sufficient to make a model robust. As a remedy, we prescribe simple modifications that can improve the robustness of PLM-based EM models. Our experiments show that while yielding superior results for in-domain generalization, our proposed model significantly improves the model robustness, compared to state-of-the-art EM models.},
    booktitle = {Proceedings of the 31st ACM International Conference on Information & Knowledge Management},
    pages = {3786–3790},
    location = {Atlanta, GA, USA},
    url = {https://doi.org/10.1145/3511808.3557673},
    doi = {10.1145/3511808.3557673},
    series = {CIKM '22},
    code={https://github.com/makbn/robem},
}

@inproceedings{frozen,
    preview = {SIGIR22.jpg},
    author = {{Yadegari}, Mostafa and {Kamalloo}, Ehsan and {Rafiei}, Davood},
    title = {Detecting Frozen Phrases in Open-Domain Question Answering},
    month = jul,
    year = {2022},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3477495.3531793},
    doi = {10.1145/3477495.3531793},
    abstract = {There is essential information in the underlying structure of words and phrases in natural language questions, and this structure has been extensively studied. In this paper, we study one particular structure, referred to as frozen phrases, that is highly expected to transfer as a whole from questions to answer passages. Frozen phrases, if detected, can be helpful in open-domain Question Answering (QA) where identifying the localized context of a given input question is crucial. An interesting question is if frozen phrases can be accurately detected. We cast the problem as a sequence-labeling task and create synthetic data from existing QA datasets to train a model. We further plug this model into a sparse retriever that is made aware of the detected phrases. Our experiments reveal that detecting frozen phrases whose presence in answer documents are highly plausible yields significant improvements in retrievals as well as in the end-to-end accuracy of open-domain QA models.},
    booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
    pages = {1990–1996},
    numpages = {7},
    keywords = {open-domain question answering, sparse retriever, information retrieval, question paraphrasing, frozen phrases},
    location = {Madrid, Spain},
    code={https://github.com/Aashena/Frozen-Phrases},
}

@article{faithdial,
  title={FaithDial: A Faithful Benchmark for Information-Seeking Dialogue},
  author={{Dziri}, Nouha and {Kamalloo}, Ehsan and {Milton}, Sivan and Zaiane, Osmar and Yu, Mo and Ponti, Edoardo and Reddy, Siva},
  journal={TACL},
  month=apr,
  year={2022},
  url={https://arxiv.org/abs/2204.10757},
  arxiv={2204.10757},
  website={https://mcgill-nlp.github.io/FaithDial/},
  code={https://github.com/McGill-NLP/FaithDial},
  preview={faithdial.png},
  abstract={The goal of information-seeking dialogue is to respond to seeker queries with natural language utterances that are grounded on knowledge sources. However, dialogue systems often produce unsupported utterances, a phenomenon known as hallucination. Dziri et al. (2022)'s investigation of hallucinations has revealed that existing knowledge-grounded benchmarks are contaminated with hallucinated responses at an alarming level (>60% of the responses) and models trained on this data amplify hallucinations even further (>80% of the responses). To mitigate this behavior, we adopt a data-centric solution and create FaithDial, a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe that FaithDial is more faithful than WoW while also maintaining engaging conversations. We show that FaithDial can serve as a training signal for: i) a hallucination critic, which discriminates whether an utterance is faithful or not, and boosts the performance by 21.1 F1 score on the BEGIN benchmark compared to existing datasets for dialogue coherence; ii) high-quality dialogue generation. We benchmark a series of state-of-the-art models and propose an auxiliary contrastive objective that achieves the highest level of faithfulness and abstractiveness based on several automated metrics. Further, we find that the benefits of FaithDial generalize to zero-shot transfer on other datasets, such as CMU-Dog and TopicalChat. Finally, human evaluation reveals that responses generated by models trained on FaithDial are perceived as more interpretable, cooperative, and engaging.},
}


@inproceedings{glitter,
    image = {YUV.png},
    title = "When Chosen Wisely, More Data Is What You Need: A Universal Sample-Efficient Strategy For Data Augmentation",
    author = "Kamalloo, Ehsan  and
      Rezagholizadeh, Mehdi  and
      Ghodsi, Ali",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.84",
    doi = "10.18653/v1/2022.findings-acl.84",
    pages = "1048--1062",
    abstract = "Data Augmentation (DA) is known to improve the generalizability of deep neural networks. Most existing DA techniques naively add a certain number of augmented samples without considering the quality and the added computational cost of these samples. To tackle this problem, a common strategy, adopted by several state-of-the-art DA methods, is to adaptively generate or re-weight augmented samples with respect to the task objective during training. However, these adaptive DA methods: (1) are computationally expensive and not sample-efficient, and (2) are designed merely for a specific setting. In this work, we present a universal DA technique, called Glitter, to overcome both issues. Glitter can be plugged into any DA method, making training sample-efficient without sacrificing performance. From a pre-generated pool of augmented samples, Glitter adaptively selects a subset of worst-case samples with maximal loss, analogous to adversarial DA. Without altering the training strategy, the task objective can be optimized on the selected subset. Our thorough experiments on the GLUE benchmark, SQuAD, and HellaSwag in three widely used training setups including consistency training, self-distillation and knowledge distillation reveal that Glitter is substantially faster to train and achieves a competitive performance, compared to strong baselines.",
    preview={glitter.png},
    code={https://github.com/huawei-noah/KD-NLP/tree/main/Glitter},
}

@inproceedings{minimax-kNN,
    title = "Not Far Away, Not So Close: Sample Efficient Nearest Neighbour Data Augmentation via {M}ini{M}ax",
    author = "Kamalloo, Ehsan  and
      Rezagholizadeh, Mehdi  and
      Passban, Peyman  and
      Ghodsi, Ali",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.309",
    doi = "10.18653/v1/2021.findings-acl.309",
    pages = "3522--3533",
    code={https://github.com/huawei-noah/KD-NLP/tree/main/Minimax-kNN},
    preview={minimax-knn.png},
    abstract={In Natural Language Processing (NLP), finding data augmentation techniques that can produce high-quality human-interpretable examples has always been challenging. Recently, leveraging kNN such that augmented examples are retrieved from large repositories of unlabelled sentences has made a step toward interpretable augmentation. Inspired by this paradigm, we introduce Minimax-kNN, a sample efficient data augmentation strategy tailored for Knowledge Distillation (KD). We exploit a semi-supervised approach based on KD to train a model on augmented data. In contrast to existing kNN augmentation techniques that blindly incorporate all samples, our method dynamically selects a subset of augmented samples that maximizes KL-divergence between the teacher and student models. This step aims to extract the most efficient samples to ensure our augmented data covers regions in the input space with maximum loss value. We evaluated our technique on several text classification tasks and demonstrated that Minimax-kNN consistently outperforms strong baselines. Our results show that Minimax-kNN requires fewer augmented examples and less computation to achieve superior performance over the state-of-the-art kNN-based augmentation techniques.},
}


@inproceedings{dziri-etal-2019-evaluating,
    title = "Evaluating Coherence in Dialogue Systems using Entailment",
    author = "Dziri, Nouha  and
      Kamalloo, Ehsan  and
      Mathewson, Kory  and
      Zaiane, Osmar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1381",
    doi = "10.18653/v1/N19-1381",
    pages = "3806--3812",
    abstract = "Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses.",
    code={https://github.com/nouhadziri/DialogEntailment},
}


@inproceedings{THRED,
    title = "Augmenting Neural Response Generation with Context-Aware Topical Attention",
    author = "Dziri, Nouha  and
      Kamalloo, Ehsan  and
      Mathewson, Kory  and
      Zaiane, Osmar",
    booktitle = "Proceedings of the First Workshop on NLP for Conversational AI (NLP4ConvAI) at ACL 2019",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4103",
    doi = "10.18653/v1/W19-4103",
    pages = "18--31",
    abstract = "Sequence-to-Sequence (Seq2Seq) models have witnessed a notable success in generating natural conversational exchanges. Notwithstanding the syntactically well-formed responses generated by these neural network models, they are prone to be acontextual, short and generic. In this work, we introduce a Topical Hierarchical Recurrent Encoder Decoder (THRED), a novel, fully data-driven, multi-turn response generation system intended to produce contextual and topic-aware responses. Our model is built upon the basic Seq2Seq model by augmenting it with a hierarchical joint attention mechanism that incorporates topical concepts and previous interactions into the response generation. To train our model, we provide a clean and high-quality conversational dataset mined from Reddit comments. We evaluate THRED on two novel automated metrics, dubbed Semantic Similarity and Response Echo Index, as well as with human evaluation. Our experiments demonstrate that the proposed model is able to generate more diverse and contextually relevant responses compared to the strong baselines.",
    code={https://github.com/nouhadziri/THRED},
    preview={THRED.png},

}

@inproceedings{toponym,
    author = {Kamalloo, Ehsan and Rafiei, Davood},
    title = {A Coherent Unsupervised Model for Toponym Resolution},
    month={apr},
    year = {2018},
    publisher = {International World Wide Web Conferences Steering Committee},
    doi = {10.1145/3178876.3186027},
    abstract = {Toponym Resolution, the task of assigning a location mention in a document to a geographic referent (i.e., latitude/longitude), plays a pivotal role in analyzing location-aware content. However, the ambiguities of natural language and a huge number of possible interpretations for toponyms constitute insurmountable hurdles for this task. In this paper, we study the problem of toponym resolution with no additional information other than a gazetteer and no training data. We demonstrate that a dearth of large enough annotated data makes supervised methods less capable of generalizing. Our proposed method estimates the geographic scope of documents and leverages the connections between nearby place names as evidence to resolve toponyms. We explore the interactions between multiple interpretations of mentions and the relationships between different toponyms in a document to build a model that finds the most coherent resolution. Our model is evaluated on three news corpora, two from the literature and one collected and annotated by us; then, we compare our methods to the state-of-the-art unsupervised and supervised techniques. We also examine three commercial products including Reuters OpenCalais, Yahoo! YQL Placemaker, and Google Cloud Natural Language API. The evaluation shows that our method outperforms the unsupervised technique as well as Reuters OpenCalais and Google Cloud Natural Language API on all three corpora; also, our method shows a performance close to that of the state-of-the art supervised method and outperforms it when the test data has 40% or more toponyms that are not seen in the training data.},
    booktitle = {Proceedings of the 2018 World Wide Web Conference (WWW)},
    pages = {1287–1296},
    numpages = {10},
    keywords = {spatial hierarchies, toponym resolution, unsupervised disambiguation, context-bound hypotheses, geolocation extraction},
    location = {Lyon, France},
    code={https://github.com/ehsk/CHF-TopoResolver},
}